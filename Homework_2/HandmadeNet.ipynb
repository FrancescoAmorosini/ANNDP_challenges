{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "handmade_net_crops_challenge2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txvrt-Uqw06-"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kewINrBOxz75"
      },
      "source": [
        "# TO RUN THIS NOTEBOOK YOU HAVE TO CONSIDER AS TRAINING DATASET DIRECTORY JUST THE BIPBIP ONE\r\n",
        "\r\n",
        "!unzip /content/drive/MyDrive/Bipbip.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQHgLlHw07A"
      },
      "source": [
        "cwd = os.getcwd()\n",
        "cwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8gNEu4UfRM8"
      },
      "source": [
        "import math\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "# FUNCTIONS FOR CROPS:\r\n",
        "\r\n",
        "def save_crops(path_img, save_dir, tipo, size=512, overlap=10):\r\n",
        "    img = Image.open(path_img)\r\n",
        "    name = os.path.normpath(path_img)\r\n",
        "    name = os.path.basename(name)\r\n",
        "    name = os.path.splitext(name)[0]\r\n",
        "    width, height = img.size\r\n",
        "    \r\n",
        "    # box =  left, upper, right, and lower\r\n",
        "    x = 0\r\n",
        "    y = 0\r\n",
        "    arrived_y = 0\r\n",
        "\r\n",
        "    while arrived_y + size <= height:\r\n",
        "        arrived_x = 0\r\n",
        "        x = 0\r\n",
        "        while arrived_x + size <= width:\r\n",
        "            box = (arrived_x, arrived_y, arrived_x + size, arrived_y + size)\r\n",
        "            one_crop = img.crop(box)\r\n",
        "            if (tipo == 'img'):\r\n",
        "              name_part = \"-\" + str(x) + \"-\" + str(y) + \".jpg\"\r\n",
        "              one_crop.save(os.path.join(save_dir, name + name_part))\r\n",
        "            else:\r\n",
        "              if (tipo == 'mask'):\r\n",
        "                name_part = \"-\" + str(x) + \"-\" + str(y) + \".png\"\r\n",
        "                one_crop.save(os.path.join(save_dir, name + name_part))\r\n",
        "              else:\r\n",
        "                print('tipo sbagliato!! ERROREE no salvataggio')\r\n",
        "            x = x+1\r\n",
        "            arrived_x = arrived_x + size - overlap\r\n",
        "        if width - arrived_x >= 1:\r\n",
        "            # creo img tutta nera\r\n",
        "            black_img = np.zeros([size,size,3],dtype=np.uint8)\r\n",
        "            black_img = Image.fromarray(black_img)\r\n",
        "            box = (arrived_x, arrived_y, width, arrived_y + size)\r\n",
        "            one_crop = img.crop(box)\r\n",
        "            black_img.paste(one_crop, (0, 0))\r\n",
        "            if (tipo == 'img'):\r\n",
        "              black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".jpg\"))\r\n",
        "            else:\r\n",
        "              if (tipo == 'mask'):\r\n",
        "                black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".png\"))\r\n",
        "              else:\r\n",
        "                print('tipo sbagliato!! ERROREE no salvataggio')\r\n",
        "        arrived_y = arrived_y + size - overlap\r\n",
        "        y = y+1\r\n",
        "                                  \r\n",
        "    x = 0\r\n",
        "    if height - arrived_y >= 1:\r\n",
        "        arrived_x = 0\r\n",
        "        while arrived_x + size <= width:\r\n",
        "            black_img = np.zeros([size,size,3],dtype=np.uint8)\r\n",
        "            black_img = Image.fromarray(black_img)\r\n",
        "            box = (arrived_x, arrived_y, arrived_x + size, height)\r\n",
        "            one_crop = img.crop(box)\r\n",
        "            black_img.paste(one_crop, (0, 0))\r\n",
        "            if (tipo == 'img'):              \r\n",
        "              black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".jpg\"))\r\n",
        "            else:\r\n",
        "              if (tipo == 'mask'):\r\n",
        "                black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".png\"))\r\n",
        "              else:\r\n",
        "                print('tipo sbagliato!! ERROREE no salvataggio')\r\n",
        "            arrived_x = arrived_x + size - overlap\r\n",
        "            x = x+1\r\n",
        "        if width - arrived_x >= 1:\r\n",
        "            # creo img tutta nera\r\n",
        "            black_img = np.zeros([size,size,3],dtype=np.uint8)\r\n",
        "            black_img = Image.fromarray(black_img)\r\n",
        "            box = (arrived_x, arrived_y, width, height)\r\n",
        "            one_crop = img.crop(box)\r\n",
        "            black_img.paste(one_crop, (0, 0))\r\n",
        "            if (tipo == 'img'):    \r\n",
        "              black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".jpg\"))\r\n",
        "            else:\r\n",
        "              if (tipo == 'mask'):\r\n",
        "                black_img.save(os.path.join(save_dir, name + \"-\" + str(x) + \"-\" + str(y) + \".png\"))\r\n",
        "              else:\r\n",
        "                print('tipo sbagliato!! ERROREE no salvataggio')\r\n",
        "                           \r\n",
        "    return\r\n",
        "def takeSecond(elem):\r\n",
        "    return elem[1]\r\n",
        "                           \r\n",
        "def crops_reconstruct(imgs_path, save_dir, final_size, size = 256, overlap = 10):\r\n",
        "    images = []\r\n",
        "    for filename in os.listdir(imgs_path):\r\n",
        "        name = os.path.splitext(filename)[0]\r\n",
        "        elem = name.split(\"-\") # ottengo [name, x, y] in stringhe\r\n",
        "        images.append(elem)\r\n",
        "    i = 0\r\n",
        "    list_same_img = images         \r\n",
        "    black_img = np.zeros([final_size[1],final_size[0],3],dtype=int)   \r\n",
        "    black_img = Image.fromarray(black_img, 'RGB')\r\n",
        "    save_img_name = list_same_img[0][0]\r\n",
        "    x = 0\r\n",
        "    y = 0\r\n",
        "                        \r\n",
        "    number_of_y = len(set([img[2] for img in list_same_img])) # valore in pos 2 Ã¨ y\r\n",
        "    for y_number in range (0, number_of_y):\r\n",
        "        x = 0\r\n",
        "        list_crops_same_row = []\r\n",
        "        for elem in list_same_img:\r\n",
        "            if elem[2] == str(y_number):\r\n",
        "                list_crops_same_row.append(elem)\r\n",
        "                        \r\n",
        "        list_crops_same_row.sort(key=takeSecond)\r\n",
        "\r\n",
        "        for crop in list_crops_same_row:        \r\n",
        "            string = crop[0] + \"-\" + crop[1] + \"-\" + crop[2] + \".png\"\r\n",
        "            take_crop = Image.open(os.path.join(imgs_path, string))\r\n",
        "            black_img.paste(take_crop, (x, y))\r\n",
        "            x = x + size - overlap\r\n",
        "    \r\n",
        "        y = y + size - overlap\r\n",
        "    #black_img.crop(final)\r\n",
        "    black_img.save(os.path.join(save_dir, save_img_name + \".png\"))\r\n",
        "    return black_img\r\n",
        "\r\n",
        "# class colors: [0,0,0], [255,255,255], [216,67,82]\r\n",
        "def make_equal(img_arr, t1=10, t2=240):\r\n",
        "    img_arr = img_arr.copy()\r\n",
        "\r\n",
        "    for i in range (0,img_arr.shape[0]):\r\n",
        "        for j in range (0,img_arr.shape[1]):\r\n",
        "            if (img_arr[i][j][2]) > t1 and img_arr[i][j][2] < t2:\r\n",
        "              if (img_arr[i][j][1]) > t1 and img_arr[i][j][1] < t2:\r\n",
        "                img_arr[i][j][0] = 216\r\n",
        "                img_arr[i][j][1] = 67\r\n",
        "                img_arr[i][j][2] = 82\r\n",
        "            if (img_arr[i][j][0]) >= t2:\r\n",
        "                if (img_arr[i][j][1]) >= t2:\r\n",
        "                    if (img_arr[i][j][2]) >= t2:\r\n",
        "                        img_arr[i][j][0] = 255\r\n",
        "                        img_arr[i][j][1] = 255\r\n",
        "                        img_arr[i][j][2] = 255\r\n",
        "            if (img_arr[i][j][0]) <= t1:\r\n",
        "                if (img_arr[i][j][1]) <= t1:\r\n",
        "                    if (img_arr[i][j][2]) <= t1:\r\n",
        "                        img_arr[i][j][0] = 0\r\n",
        "                        img_arr[i][j][1] = 0\r\n",
        "                        img_arr[i][j][2] = 0\r\n",
        "\r\n",
        "    return img_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lP__xyafszP"
      },
      "source": [
        "from PIL import Image\r\n",
        "\r\n",
        "test_dir = os.path.join(cwd,\"Test_Dev\")\r\n",
        "\r\n",
        "crops = os.path.join(cwd, \"Crops\")\r\n",
        "if not os.path.exists(crops):\r\n",
        "    os.makedirs(crops)\r\n",
        "save_crops_dir = os.path.join(cwd, \"Crops/Images\")\r\n",
        "if not os.path.exists(save_crops_dir):\r\n",
        "    os.makedirs(save_crops_dir)\r\n",
        "save_crops_masks_dir = os.path.join(cwd, \"Crops/Masks\")\r\n",
        "if not os.path.exists(save_crops_masks_dir):\r\n",
        "    os.makedirs(save_crops_masks_dir)\r\n",
        "    \r\n",
        "json_dir = os.path.join(cwd, \"CONTENT\")\r\n",
        "if not os.path.exists(json_dir):\r\n",
        "    os.makedirs(json_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Q_b04igi7B"
      },
      "source": [
        "import os\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "def rle_encode(img):\r\n",
        "    '''\r\n",
        "    img: numpy array, 1 - foreground, 0 - background\r\n",
        "    Returns run length as string formatted\r\n",
        "    '''\r\n",
        "    pixels = img.flatten()\r\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\r\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n",
        "    runs[1::2] -= runs[::2]\r\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mljvKpY5Lt0u"
      },
      "source": [
        "# croppiamo le immagini del dataset e le salviamo in Crops/Images, che diventerÃ  il nuovo dataset di riferimento\r\n",
        "for (root,dirs,files) in os.walk('/content/Bipbip/Haricot/Images'):\r\n",
        "  for file in files:\r\n",
        "    print(root + file)\r\n",
        "    # tipo = img cosÃ¬ vengono salvate con estensione .jpg come le originali\r\n",
        "    save_crops(root + '/' + file, '/content/Crops/Images', size=512, overlap=20, tipo = 'img')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdFyV5ZPRGeD"
      },
      "source": [
        "# idem per le maschere\r\n",
        "for (root,dirs,files) in os.walk('/content/Bipbip/Haricot/Masks'):\r\n",
        "  for file in files:\r\n",
        "    print(root + file)\r\n",
        "    # tipo = mask cosÃ¬ vengono salvate con estensione .png come le originali\r\n",
        "    save_crops(root + '/' + file, '/content/Crops/Masks', size=512, overlap=20, tipo = 'mask')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne5_NGPLh0NL"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Test_Dev.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c5vQJlOw07A"
      },
      "source": [
        "# ImageDataGenerator\n",
        "# ------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "apply_data_augmentation = True\n",
        "\n",
        "if apply_data_augmentation:\n",
        "    # data augumentation for the rgb image\n",
        "    train_img_data_gen = ImageDataGenerator(rotation_range=30,\n",
        "                                            width_shift_range=10,\n",
        "                                            height_shift_range=10,\n",
        "                                            zoom_range=0.3,\n",
        "                                            horizontal_flip=True,\n",
        "                                            vertical_flip=True,\n",
        "                                            fill_mode='reflect',\n",
        "                                            rescale=1./255)\n",
        "    # data augumentation for the mask \n",
        "    # non divido per 255 perchÃ¨ non devono essere normalizzate\n",
        "    train_mask_data_gen = ImageDataGenerator(rotation_range=30,\n",
        "                                             width_shift_range=10,\n",
        "                                             height_shift_range=10,\n",
        "                                             zoom_range=0.3,\n",
        "                                             horizontal_flip=True,\n",
        "                                             vertical_flip=True,\n",
        "                                             fill_mode='reflect')\n",
        "else:\n",
        "    train_img_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "    # idem qui, no divisione\n",
        "    train_mask_data_gen = ImageDataGenerator()\n",
        "\n",
        "# Create validation and test ImageDataGenerator objects\n",
        "# e anche idem qui\n",
        "valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "valid_mask_data_gen = ImageDataGenerator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uzXzhfvw07A"
      },
      "source": [
        "def read_rgb_mask(img_path, shape):\n",
        "    '''\n",
        "    img_path: path to the mask file\n",
        "    Returns the numpy array containing target values\n",
        "    '''\n",
        "\n",
        "    mask_img = Image.open(img_path)\n",
        "    \n",
        "    #mask_img = mask_img.resize(shape, resample=Image.NEAREST)\n",
        "    mask_arr = np.array(mask_img)\n",
        "\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [254, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "\n",
        "    return new_mask_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK54YNSLw07A"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
        "               preprocessing_function=None, out_shape=[512, 512]):\n",
        "        if which_subset == 'training':\n",
        "            subset_file = os.path.join(dataset_dir, 'Splits', 'train.txt')\n",
        "        elif which_subset == 'validation':\n",
        "            subset_file = os.path.join(dataset_dir, 'Splits', 'val.txt')\n",
        "\n",
        "        with open(subset_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        subset_filenames = []\n",
        "        for line in lines:\n",
        "            subset_filenames.append(line.strip()) \n",
        "\n",
        "            self.which_subset = which_subset\n",
        "            self.dataset_dir = dataset_dir\n",
        "            self.subset_filenames = subset_filenames\n",
        "            self.img_generator = img_generator\n",
        "            self.mask_generator = mask_generator\n",
        "            self.preprocessing_function = preprocessing_function\n",
        "            self.out_shape = out_shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # read img\n",
        "        curr_filename = self.subset_filenames[index]\n",
        "        img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\n",
        "        mask = read_rgb_mask(os.path.join(self.dataset_dir, 'Masks', curr_filename + '.png'), self.out_shape)\n",
        "        img = img.resize(self.out_shape) # questo resize nel nostro caso Ã¨ superfluo ma lo lasciamo per sicurezza\n",
        "        # superfluo perchÃ¨ abbiamo giÃ  croppato le immagini con la size giusta, quindi non avviene alcun resize\n",
        "\n",
        "        img_arr = np.array(img)\n",
        "        mask_arr = np.array(mask)\n",
        "        \n",
        "        mask_arr[mask_arr == 255] = 0  \n",
        "\n",
        "        mask_arr = np.expand_dims(mask_arr, -1)\n",
        "\n",
        "        if self.which_subset == 'training':\n",
        "            if self.img_generator is not None and self.mask_generator is not None:\n",
        "                # Perform data augmentation\n",
        "                # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
        "                # and we can apply it to the image using apply_transform\n",
        "                img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
        "                mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
        "                img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "                # ImageDataGenerator use bilinear interpolation for augmenting the images.\n",
        "                # Thus, when applied to the masks it will output 'interpolated classes', which\n",
        "                # is an unwanted behaviour. As a trick, we can transform each class mask \n",
        "                # separately and then we can cast to integer values (as in the binary segmentation notebook).\n",
        "                # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n",
        "                out_mask = np.zeros_like(mask_arr)\n",
        "                for c in np.unique(mask_arr):\n",
        "                    if c > 0:\n",
        "                        curr_class_arr = np.float32(mask_arr == c)\n",
        "                        curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
        "                        # from [0, 1] to {0, 1}\n",
        "                        curr_class_arr = np.uint8(curr_class_arr)\n",
        "                        # recover original class\n",
        "                        curr_class_arr = curr_class_arr * c \n",
        "                        out_mask += curr_class_arr\n",
        "        else:\n",
        "            out_mask = mask_arr\n",
        "\n",
        "        if self.preprocessing_function is not None:\n",
        "            img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "        return img_arr, np.float32(out_mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acB5eX73w07B"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "dataset_dir = \"/content/Crops\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjTc-mcQw07B"
      },
      "source": [
        "# andiamo a definire la divisione train val nei file txt\n",
        "\n",
        "\"\"\"per ogni file in haricot prendo il nome del file, ci tolgo .jpg e lo metto in una lista\n",
        "poi prendo la lista e faccio shuffle\n",
        "creo txt train e val (e li svuoto)\n",
        "poi prendo i primi 80 % elem della lista e li scrivo nel txt train, il 20 % in txt val\"\"\"\n",
        "\n",
        "# 80:100=x:len x=lenx80:100\n",
        "import os\n",
        "import random\n",
        "\n",
        "list_img_names = []\n",
        "for filename in os.listdir(os.path.join(dataset_dir, 'Images')):\n",
        "    img_name = os.path.splitext(filename)[0]\n",
        "    #print(img_name)\n",
        "    list_img_names.append(img_name)\n",
        "\n",
        "random.shuffle(list_img_names)\n",
        "lenght = len(list_img_names)\n",
        "\n",
        "lenght_train = 80\n",
        "lenght_train = int((lenght * lenght_train)/100)\n",
        "\n",
        "if not os.path.exists(os.path.join(dataset_dir, 'Splits')):\n",
        "  os.mkdir(os.path.join(dataset_dir, 'Splits'))\n",
        "\n",
        "train_file= open(os.path.join(dataset_dir, 'Splits/train.txt'),\"w+\")\n",
        "train_file.truncate(0)\n",
        "val_file= open(os.path.join(dataset_dir, 'Splits/val.txt'),\"w+\")\n",
        "val_file.truncate(0)\n",
        "\n",
        "for i in range (0, lenght_train-1):\n",
        "    elem = list_img_names[0]\n",
        "    train_file.write(elem + \"\\n\")\n",
        "    list_img_names.remove(elem)\n",
        "\n",
        "train_file.close()\n",
        "\n",
        "lenght = len(list_img_names)\n",
        "for i in range (0, lenght):\n",
        "    elem = list_img_names[0]\n",
        "    val_file.write(elem + \"\\n\")\n",
        "    list_img_names.remove(elem)\n",
        "\n",
        "val_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UqNhKoETw07D"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
        "\n",
        "img_h = 512\n",
        "img_w = 512\n",
        "\n",
        "\n",
        "dataset = CustomDataset(dataset_dir, 'training', img_generator=train_img_data_gen, mask_generator=train_mask_data_gen, preprocessing_function=preprocess_input)\n",
        "\n",
        "dataset_valid = CustomDataset(dataset_dir, 'validation', img_generator=valid_img_data_gen, mask_generator=valid_mask_data_gen, preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxjube4mw07D"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1])) #1?????\n",
        "\n",
        "bs = 16\n",
        "\n",
        "train_dataset = train_dataset.batch(bs)\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n",
        "\n",
        "valid_dataset = valid_dataset.batch(bs)\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HKX6Hmkw07E"
      },
      "source": [
        "# Let's test data generator\n",
        "# -------------------------\n",
        "import time\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Assign a color to each class\n",
        "evenly_spaced_interval = np.linspace(0, 1, 2)\n",
        "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
        "\n",
        "iterator = iter(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmHD2f32w07E"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "augmented_img, target = next(iterator)\n",
        "augmented_img = augmented_img[0]   # First element\n",
        "augmented_img = augmented_img  # denormalize\n",
        "\n",
        "target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n",
        "\n",
        "print(np.unique(target))\n",
        "\n",
        "target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "\n",
        "target_img[np.where(target == 0)] = [0, 0, 0]\n",
        "for i in range(1, 3):\n",
        "    target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\n",
        "\n",
        "ax[0].imshow(np.uint8(augmented_img))\n",
        "ax[1].imshow(np.uint8(target_img))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7oMHBw0w07F"
      },
      "source": [
        "# CONVOLUTIONAL NEURAL NETWORK: encode - decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbRt8a-Tw07F"
      },
      "source": [
        "def create_model(depth, start_f, num_classes):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Encoder (the same we use for classification)\n",
        "    # -------\n",
        "    for i in range(depth): # depth = num conv layers che otterrÃ²\n",
        "        # per ogni conv block alterno conv con poi activation e maxpool\n",
        "        \n",
        "        if i == 0:\n",
        "            \"\"\"if dynamic_input_shape:\n",
        "                input_shape = [None, None, 3]\n",
        "            else:\"\"\"\n",
        "            input_shape = [img_h, img_w, 3]\n",
        "        else:\n",
        "            input_shape=[None]\n",
        "        \n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f, \n",
        "                                         kernel_size=(3, 3),\n",
        "                                         strides=(1, 1),\n",
        "                                         padding='same',\n",
        "                                         input_shape=input_shape))\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "        start_f *= 2\n",
        "\n",
        "    # Bottleneck\n",
        "    # which can be another conv with another relu\n",
        "    model.add(tf.keras.layers.Conv2D(filters=start_f, \n",
        "                                     kernel_size=(3, 3), \n",
        "                                     strides=(1, 1), \n",
        "                                     padding='same'))\n",
        "    model.add(tf.keras.layers.ReLU())\n",
        "    \n",
        "    start_f = start_f // 2\n",
        "    # i start to decrease the number of feature at each layer -> start decoding\n",
        "        \n",
        "    # Decoder\n",
        "    # -------\n",
        "    # i have to reconstruct the input conv\n",
        "    for i in range(depth):\n",
        "        # same depth of before, i'm mirroring\n",
        "        # applico upsampling, i want to double my input dimension and use bilinear interpol\n",
        "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n",
        "        # i can apply non linearity to a conv layer to trasform the upsampled feature to a some dense features: and this is done by appliyng convolution:\n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
        "                                         kernel_size=(3, 3),\n",
        "                                         strides=(1, 1),\n",
        "                                         padding='same'))\n",
        "        # final activation\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "\n",
        "        start_f = start_f // 2\n",
        "\n",
        "    # Prediction Layer\n",
        "    # ----------------\n",
        "    # classification layer: conv2d. Kernel di 1x1xinput_channels\n",
        "    # this is performing a fully connected layer for each pixel n the channel dimension\n",
        "    # so is like convolving a fully connected layer to the input volume\n",
        "    # then for each pixel i can obtain an output prediction\n",
        "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n",
        "                                     kernel_size=(1, 1),\n",
        "                                     strides=(1, 1),\n",
        "                                     padding='same',\n",
        "                                     activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aay-HAOUw07F"
      },
      "source": [
        "model = create_model(depth=5, \n",
        "                     start_f=8, #8 \n",
        "                     num_classes=3)\n",
        "\n",
        "# Visualize created model as a table\n",
        "model.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "# model.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQXeEFOJw07F"
      },
      "source": [
        "# prepare model for training\n",
        "\n",
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
        "# learning rate\n",
        "lr = 1e-4 #3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Here we define the intersection over union for each class in the batch.\n",
        "# Then we compute the final iou as the mean over classes\n",
        "def meanIoU(y_true, y_pred, num_classes=3):\n",
        "    # get predicted class from softmax\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
        "\n",
        "    per_class_iou = []\n",
        "\n",
        "    for i in range(1, num_classes): # exclude the background class 0\n",
        "      # Get prediction and target related to only a single class (i)\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
        "    \n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "      per_class_iou.append(iou)\n",
        "\n",
        "    return tf.reduce_mean(per_class_iou)\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "metrics = [meanIoU] #['accuracy', meanIoU] \n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPr-epM1w07F"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "exps_dir = os.path.join(cwd, 'drive/My Drive/Keras4/', 'multiclass_segmentation_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "# ----------------\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AElXuFMTw07F"
      },
      "source": [
        "print(len(dataset))\n",
        "\n",
        "history = model.fit(x=train_dataset,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=len(dataset)/bs,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(dataset_valid)/bs, \n",
        "          callbacks=callbacks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ojYaoLw07F"
      },
      "source": [
        "# MODEL TEST\n",
        "# se serve fai load weights\n",
        "model.load_weights('/content/drive/MyDrive/Keras4/multiclass_segmentation_experiments/CNN_Dec21_07-52-14/ckpts/cp_43.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFwxMOfs5UI4"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Test_Dev.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYRM4-GCw07F"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "test_dir = os.path.join(cwd,\"Test_Dev\")\n",
        "\n",
        "crops = os.path.join(cwd, \"Crops_Test\")\n",
        "if not os.path.exists(crops):\n",
        "    os.makedirs(crops)\n",
        "\n",
        "# qui verrasso salvate i crops del test set\n",
        "save_crops_dir = os.path.join(cwd, \"Crops_Test/Images\")\n",
        "if not os.path.exists(save_crops_dir):\n",
        "    os.makedirs(save_crops_dir)\n",
        "\n",
        "# qui verrasso salvate le maschere ottenute dalla rete sui crop del test set\n",
        "save_crops_masks_dir = os.path.join(cwd, \"Crops_Test/Masks\")\n",
        "if not os.path.exists(save_crops_masks_dir):\n",
        "    os.makedirs(save_crops_masks_dir)\n",
        "\n",
        "# in mask_reconstructed potrai vedere le maschere di output ricostruite\n",
        "save_final_masks_dir = os.path.join(cwd, \"Crops_Test/Masks_reconstructed\")\n",
        "if not os.path.exists(save_final_masks_dir):\n",
        "    os.makedirs(save_final_masks_dir)\n",
        "\n",
        "# qui verrÃ  salvato il json\n",
        "json_dir = os.path.join(cwd, \"CONTENT\")\n",
        "if not os.path.exists(json_dir):\n",
        "    os.makedirs(json_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKxp2O-cw07F"
      },
      "source": [
        "from pathlib import Path\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "i_file = 0\n",
        "test_dir = os.path.join(cwd,\"Test_Dev\")\n",
        "submission_dict = {}\n",
        "\n",
        "for subdir, dirs, files in os.walk(test_dir):\n",
        "    for file in files:\n",
        "        name_img = os.path.splitext(file)[0]\n",
        "        img_path = os.path.join(subdir, file)\n",
        "        img = Image.open(os.path.join(subdir, file))\n",
        "        w, h = img.size\n",
        "        shape_img = (w,h)\n",
        "        path = Path(subdir)\n",
        "\n",
        "        # stiamo lavorando solo con l'haricot dataset, quindi mando solo quello in input alla rete\n",
        "        if (str(path.parent.parent).split(\"/\")[-1] == \"Bipbip\") & (str(path.parent).split(\"/\")[-1] == \"Haricot\") :\n",
        "            print(\"ENTRO\")\n",
        "\n",
        "            crop_dir = os.path.join(save_crops_dir, name_img)\n",
        "            if not os.path.exists(crop_dir):\n",
        "                os.makedirs(crop_dir)\n",
        "\n",
        "            #CREO CROPS IMMAGINE \n",
        "            crop_mask_img_dir = os.path.join(save_crops_masks_dir, name_img)\n",
        "            if not os.path.exists(crop_mask_img_dir):\n",
        "                os.makedirs(crop_mask_img_dir)\n",
        "            save_crops(img_path, save_dir = crop_dir, size = 512, overlap = 0, tipo = 'img')\n",
        "\n",
        "            for c_subdir, c_dirs, c_files in os.walk(crop_dir):\n",
        "                for crop in c_files:\n",
        "                    name_crop = crop\n",
        "                    name_save_crop = os.path.splitext(name_crop)[0]\n",
        "                    crop_img = Image.open(os.path.join(crop_dir, crop))\n",
        "                    size = crop_img.size[0]\n",
        "                    crop = np.asarray(crop_img)\n",
        "                    #VGG PREPROCESSING\n",
        "                    crop = preprocess_input(crop)\n",
        "\n",
        "                    #PREDICO SU CROP\n",
        "                    out_sigmoid = model.predict(x=tf.expand_dims(crop, 0))              \n",
        "                    #non faccio resize\n",
        "                    mask_arr = tf.argmax(out_sigmoid, -1)[0, ...]\n",
        "                    mask_arr = np.array(mask_arr)\n",
        "                    \n",
        "                    crop_mask_img = np.zeros([size,size,3],dtype=np.uint8)\n",
        "                    for i in range (0, size):\n",
        "                        for j in range (0, size):\n",
        "                            if mask_arr[i][j] == 0:\n",
        "                                crop_mask_img[i][j] = [0,0,0]\n",
        "                            if mask_arr[i][j] == 1:\n",
        "                                crop_mask_img[i][j] = [255,255,255]\n",
        "                            if mask_arr[i][j] == 2:\n",
        "                                crop_mask_img[i][j] = [216, 67, 82]\n",
        "                        \n",
        "                    crop_mask_img = Image.fromarray(crop_mask_img)\n",
        "\n",
        "                    crop_mask_img.save(os.path.join(crop_mask_img_dir, name_save_crop +'.png'))\n",
        "                    \n",
        "            #esco dal ciclo dei crop, perchÃ¨ ora ho tutte le maschere in \"crop_mask_img_dir\" che vanno ricomposte\n",
        "            final_mask = crops_reconstruct(crop_mask_img_dir, save_dir = save_final_masks_dir, final_size = (2048, 1536), size = 512, overlap = 0)\n",
        "            print(\"SALVO TOT\") \n",
        "            final_mask_arr = np.asarray(final_mask)\n",
        "            shape_final_size = final_mask.size\n",
        "            #USO MAKE EQUAL\n",
        "            final_mask_arr = make_equal(final_mask_arr, t1=10, t2=230)\n",
        "            mask_arr = np.zeros([shape_final_size[1],shape_final_size[0]])\n",
        "            for i in range (0, final_mask_arr.shape[0]):\n",
        "                for j in range (0, final_mask_arr.shape[1]):\n",
        "                    if (final_mask_arr[i][j][0] == 255): \n",
        "                        mask_arr[i][j] = 1\n",
        "                    if (final_mask_arr[i][j][0] == 216):\n",
        "                        mask_arr[i][j] = 2\n",
        "\n",
        "            submission_dict[name_img] = {}\n",
        "            submission_dict[name_img]['shape'] = mask_arr.shape\n",
        "            submission_dict[name_img]['team'] = 'Bipbip'\n",
        "            submission_dict[name_img]['crop'] = 'Haricot'\n",
        "            submission_dict[name_img]['segmentation'] = {}\n",
        "\n",
        "            rle_encoded_crop = rle_encode(mask_arr == 1)\n",
        "            rle_encoded_weed = rle_encode(mask_arr == 2)\n",
        "\n",
        "            submission_dict[name_img]['segmentation']['crop'] = rle_encoded_crop\n",
        "            submission_dict[name_img]['segmentation']['weed'] = rle_encoded_weed\n",
        "\n",
        "\n",
        "            with open(os.path.join(json_dir,'submission.json'), 'w') as f:\n",
        "                i_file = i_file+1\n",
        "                json.dump(submission_dict, f)\n",
        "            print(\"json per 1 fatto\")\n",
        "\n",
        "        else:\n",
        "            mask_arr = np.ones(shape_img) # per tutti gli altri dataset creo maschera con tutti 1, no vere predizioni\n",
        "            submission_dict[name_img] = {}\n",
        "            submission_dict[name_img]['shape'] = mask_arr.shape\n",
        "            submission_dict[name_img]['team'] = str(path.parent.parent).split(\"/\")[-1]\n",
        "            submission_dict[name_img]['crop'] = str(path.parent).split(\"/\")[-1]\n",
        "            submission_dict[name_img]['segmentation'] = {}\n",
        "\n",
        "            rle_encoded_crop = rle_encode(mask_arr == 1)\n",
        "            rle_encoded_weed = rle_encode(mask_arr == 2)\n",
        "            submission_dict[name_img]['segmentation']['crop'] = rle_encoded_crop\n",
        "            submission_dict[name_img]['segmentation']['weed'] = rle_encoded_weed\n",
        "\n",
        "\n",
        "            with open(os.path.join(json_dir,'submission.json'), 'w') as f:\n",
        "                i_file = i_file+1\n",
        "                json.dump(submission_dict, f)\n",
        "f.close()\n",
        "\n",
        "print(\"FINE\")\n",
        "print(i_file)\n",
        "      \n",
        "      \n",
        "          \n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2-ZrS5c6bOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}