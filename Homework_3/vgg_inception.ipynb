{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import json\n","from PIL import Image\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","SEED=1234\n","tf.random.set_seed(SEED)"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["dataset_dir = \"../input/anndl-2020-vqa/VQA_Dataset\"\n","\n","imgs_path = os.path.join(dataset_dir, \"Images\")\n","train_json_path = os.path.join(dataset_dir, \"train_questions_annotations.json\")\n","test_json_path = os.path.join(dataset_dir, \"test_questions.json\")"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# andiamo a definire la divisione train val nei file txt\n","\n","\"\"\"per ogni ogg json prendo obj id (primo elemento) lo metto in una lista\n","poi prendo la lista e faccio shuffle\n","creo txt train e val (e li svuoto)\n","poi prendo i primi 80 % elem della lista e li scrivo nel txt train, il 20 % in txt val\"\"\"\n","\n","# 80:100=x:len x=lenx80:100\n","import os\n","import random\n","import json\n","\n","list_obj_id = [] \n","with open(train_json_path) as data_file:    \n","    data = json.load(data_file)\n","data_file.close()\n","#i = 0\n","for item in data.items():\n","    #i = i+1\n","    obj_value = item[0]\n","    list_obj_id.append(obj_value)\n","    #print(obj_value)\n","\n","random.shuffle(list_obj_id)\n","lenght = len(list_obj_id)\n","#print(lenght, i)\n","\n","lenght_train = 80\n","lenght_train = int((lenght * lenght_train)/100)\n","\n","split_dir = os.path.join(\"./\", \"Splits\")\n","\n","if not os.path.isdir(split_dir):\n","    os.mkdir(split_dir)\n","\n","train_file= open(os.path.join(\"./\", 'Splits/train.txt'),\"w+\")\n","train_file.truncate(0)\n","val_file= open(os.path.join(\"./\", 'Splits/val.txt'),\"w+\")\n","val_file.truncate(0)\n","\n","for i in range (0, lenght_train-1):\n","    elem = list_obj_id[0]\n","    train_file.write(elem + \"\\n\")\n","    list_obj_id.remove(elem)\n","\n","train_file.close()\n","\n","lenght = len(list_obj_id)\n","for i in range (0, lenght):\n","    elem = list_obj_id[0]\n","    val_file.write(elem + \"\\n\")\n","    list_obj_id.remove(elem)\n","\n","val_file.close()"],"execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#class definition\n","classes = {\n","    '0': 0,\n","    '1': 1,\n","    '2': 2,\n","    '3': 3,\n","    '4': 4,\n","    '5': 5,\n","    'apple': 6,\n","    'baseball': 7,\n","    'bench': 8,\n","    'bike': 9,\n","    'bird': 10,\n","    'black': 11,\n","    'blanket': 12,\n","    'blue': 13,\n","    'bone': 14,\n","    'book': 15,\n","    'boy': 16,\n","    'brown': 17,\n","    'cat': 18,\n","    'chair': 19,\n","    'couch': 20,\n","    'dog': 21,\n","    'floor': 22,\n","    'food': 23,\n","    'football': 24,\n","    'girl': 25,\n","    'grass': 26,\n","    'gray': 27,\n","    'green': 28,\n","    'left': 29,\n","    'log': 30,\n","    'man': 31,\n","    'monkey bars': 32,\n","    'no': 33,\n","    'nothing': 34,\n","    'orange': 35,\n","    'pie': 36,\n","    'plant': 37,\n","    'playing': 38,\n","    'red': 39,\n","    'right': 40,\n","    'rug': 41,\n","    'sandbox': 42,\n","    'sitting': 43,\n","    'sleeping': 44,\n","    'soccer': 45,\n","    'squirrel': 46,\n","    'standing': 47,\n","    'stool': 48,\n","    'sunny': 49,\n","    'table': 50,\n","    'tree': 51,\n","    'watermelon': 52,\n","    'white': 53,\n","    'wine': 54,\n","    'woman': 55,\n","    'yellow': 56,\n","    'yes': 57\n","}\n","\n","N_CLASSES = len(classes)"],"execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Preprocessing"]},{"metadata":{},"cell_type":"markdown","source":["Here we do some preprocessing on the questions/answers sentences using a Tokenizer to encode them."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with open(train_json_path) as data_file:    \n","    data = json.load(data_file)         \n","data_file.close()\n","\n","with open(test_json_path) as dataTest_file:    \n","    data_test = json.load(dataTest_file)         \n","dataTest_file.close()\n","\n","questions_sentences = [] # input encoder \n","answers_sentences = [] # target sentences\n","    \n","for item in data.items():\n","    question = item[1][\"question\"]\n","    question = question.replace(\"?\", \"\")\n","    answer = item[1][\"answer\"]\n","    questions_sentences.append(question)\n","    \n","for item_test in data_test.items():\n","    question = item[1][\"question\"]\n","    question = question.replace(\"?\", \"\")\n","    answer = item[1][\"answer\"]\n","    questions_sentences.append(question)\n","\n","# QUESTIONS PAD\n","question_tokenizer = Tokenizer()\n","question_tokenizer.fit_on_texts(questions_sentences)\n","question_tokenized = question_tokenizer.texts_to_sequences(questions_sentences)\n","question_wtoi = question_tokenizer.word_index\n","max_question_length = max(len(sentence) for sentence in question_tokenized)"],"execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from PIL import Image\n","\n","class CustomDataset_(tf.keras.utils.Sequence):\n","    def __init__(self, dataset_dir, which_subset, json_file, classes, n_classes, \n","                 question_tokenizer, max_question_length):\n","       \n","        if which_subset == 'training':\n","            subset_file = os.path.join('Splits', 'train.txt')\n","        elif which_subset == 'validation':\n","            subset_file = os.path.join('Splits', 'val.txt')\n","        \n","        with open(subset_file, 'r') as f:\n","            lines = f.readlines()\n","            \n","        subset_filenames = []\n","        \n","        for line in lines:\n","            subset_filenames.append(line.strip()) \n","            self.which_subset = which_subset\n","            self.dataset_dir = dataset_dir \n","            self.subset_filenames = subset_filenames\n","\n","        with open(json_file) as data_file:    \n","            data = json.load(data_file)         \n","        data_file.close()\n","        \n","        data_copy=data.copy()\n","        data_sub={k: data_copy[k] for k in subset_filenames}\n","        \n","        for item in data_sub.items():\n","            question = item[1][\"question\"]\n","            question = question.replace(\"?\", \"\")\n","            answer = item[1][\"answer\"]\n","            questions_sentences.append(question)\n","            answers_sentences.append(answer)\n","        \n","        question_tokenized = question_tokenizer.texts_to_sequences(questions_sentences)\n","        question_wtoi = question_tokenizer.word_index\n","        question_padded = pad_sequences(question_tokenized, maxlen=max_question_length)\n","      \n","        for e,item in enumerate(data_sub.items()):\n","            data_sub[item[0]]['question']=question_padded[e]\n","            data_sub[item[0]]['answer']=answers_sentences[e]\n","        \n","        self.classes = classes\n","        self.n_classes = n_classes\n","        self.data = data\n","        self.data_sub=data_sub\n","        self.question_tokenized = question_tokenized\n","        self.max_question_length = max_question_length\n","        self.question_tokenizer=question_tokenizer\n","\n","    def __len__(self):\n","        return len(self.subset_filenames)\n","\n","    def __getitem__(self, index):\n","        curr_filename = self.subset_filenames[index]\n","        obj = self.data_sub[curr_filename]\n","\n","        image_id = obj['image_id']\n","        image_name = image_id + \".png\"\n","        img = Image.open(os.path.join(self.dataset_dir, image_name))\n","        img = img.convert('RGB')\n","        img_arr = np.asarray(img)\n","        img_arr=img_arr[None,...]\n","\n","        question_tokenized = obj['question']\n","        question_tokenized = question_tokenized[None,...]\n","         \n","        answer = obj['answer']\n","        answer_num = self.classes[answer]\n","        n= np.zeros(1)\n","        n[0] = answer_num\n","        answer = n[None,...]\n","\n","        return (np.float32(question_tokenized), np.float32(img_arr)), np.float32(answer) \n"],"execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["In CustomDataset_ we generate our tuples of question-image and answer to train our network. We have used a dictionary with question, image_id and answer as key and the question_encoded, the image_id and the answer_label as values."]},{"metadata":{},"cell_type":"markdown","source":["# Dataset Creation"]},{"metadata":{},"cell_type":"markdown","source":["We create our training set and validation set similarly as we have done in the segmentation challenge, using the CustomDataset_ class."]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","dataset_dir = \"../input/anndl-2020-vqa/VQA_Dataset\"\n","\n","\n","img_h = 400\n","img_w = 700\n","num_classes=58\n","\n","dataset = CustomDataset_(imgs_path, 'training', train_json_path, classes, num_classes, \n","                         question_tokenizer, max_question_length)\n","\n","dataset_valid = CustomDataset_(imgs_path, 'validation', train_json_path, classes, num_classes,\n","                            question_tokenizer, max_question_length)\n","\n"],"execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=((tf.int32, tf.float32), tf.float32),\n","                                               output_shapes=(([None, dataset.max_question_length], [None, img_h, img_w, 3]), [None, 1]))#None,img_h, img_w, 3\n","\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=((tf.int32, tf.float32), tf.float32),\n","                                               output_shapes=(([None, dataset_valid.max_question_length], [None, img_h, img_w, 3]), [None, 1]))\n","\n","valid_dataset = valid_dataset.repeat()\n","\n","train_dataset\n","\n","\n","MAX_LENGTH=dataset.max_question_length\n","print(MAX_LENGTH)\n","\n","words_number=(len(question_wtoi)+1)"],"execution_count":25,"outputs":[{"output_type":"stream","text":"21\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Model(s)"]},{"metadata":{},"cell_type":"markdown","source":["In the vqa_model_1 we uses the fine-tuning technique, in particular we use the VGG as pretrained model and a stack of two bidirectional lstm as RNN (as in the model with the custom CNN). While in vqa_model_2 we use Inception and a Gap layer at the end (for the CNN part)."]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","import numpy as np\n","import json\n","import cv2\n","from keras import backend as K\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def vqa_model_1(img_h,img_w,MAX_LENGTH,words_number):\n","    base_model = tf.keras.applications.VGG16(input_shape=(img_h, img_w, 3), include_top=False, weights='imagenet')\n","    finetuning = True\n","\n","    if finetuning:\n","        freeze_until = 15 # layer from which we want to fine-tune\n","\n","        for layer in base_model.layers[:freeze_until]:\n","            layer.trainable = False\n","\n","    vision_model = tf.keras.models.Sequential()\n","\n","    vision_model.add(base_model)\n","\n","    vision_model.add(tf.keras.layers.Dropout(0.1))\n","\n","    vision_model.add(tf.keras.layers.Dense(1024))\n","    vision_model.add(tf.keras.layers.Flatten())\n","    image_input = tf.keras.layers.Input(shape=(img_h, img_w, 3))\n","    encoded_image = vision_model(image_input)\n","\n","    # Define RNN for language input\n","    question_input = tf.keras.layers.Input(shape=[MAX_LENGTH])\n","    embedded_question = tf.keras.layers.Embedding(input_dim=words_number, output_dim=512, input_length=MAX_LENGTH)(question_input)\n","    encoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.3, return_sequences=True, recurrent_dropout=0.1, unroll=True))(embedded_question)\n","    encoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.1, recurrent_dropout=0.1, unroll=True))(encoded_question)\n","    # Combine CNN and RNN to create the final model\n","    merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n","    output = tf.keras.layers.Dense(128)(merged)\n","    output = tf.keras.layers.Dropout(0.1)(output)\n","    output = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(output)\n","    vqa_model_1 = tf.keras.models.Model(inputs=[question_input, image_input], outputs=output)\n","    \n","    vqa_model_1.summary()\n","    return vqa_model_1"],"execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["EMBEDDING_SIZE = 128\n","\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Dropout, Input, Flatten, Dense\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","\n","def vqa_model_2(img_w, img_h,max_length,q_wtoi):\n","    # CNN\n","    inputs_cnn = Input((img_w, img_h, 3))    \n","    inc = InceptionV3(include_top=False, weights='imagenet',input_shape=(400, 700, 3))(inputs_cnn)\n","    gap = GlobalAveragePooling2D()(inc)\n","        \n","    # LSTM\n","    encoder_input = tf.keras.Input(shape=[max_length])\n","    embedding = tf.keras.layers.Embedding(q_wtoi, EMBEDDING_SIZE, input_length=max_length) (encoder_input)\n","    encoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.3, return_sequences=True, recurrent_dropout=0.1, unroll=True))(embedding)\n","    encoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.1, recurrent_dropout=0.1, unroll=True))(encoded_question)\n","    \n","    concat_layer = concatenate([gap, encoded_question])\n","    outputs = Dense(units=58, activation='softmax') (concat_layer)\n","    \n","    model = Model(inputs=[encoder_input,inputs_cnn], outputs=[outputs])\n","    return model"],"execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Set VQA_1 to true if you want to train vqa_model_1, to false if you want to train vqa_model_2."]},{"metadata":{"trusted":true},"cell_type":"code","source":["VQA_1=False\n","\n","if VQA_1:\n","    model=vqa_model_1(400, 700,MAX_LENGTH,words_number)\n","else:\n","    model=vqa_model_2(400, 700,MAX_LENGTH,words_number)"],"execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","loss =tf.keras.losses.SparseCategoricalCrossentropy()\n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Validation metrics\n","# ------------------\n","\n","metrics = ['accuracy']\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import os\n","from datetime import datetime\n","\n","bs=32\n","\n","cwd = os.getcwd()\n","\n","exps_dir = os.path.join(cwd, 'vqa_exps')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'exp'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# ----------------\n","\n","# Visualize Learning on Tensorboard\n","# ---------------------------------\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)  # if 1 shows weights histograms\n","callbacks.append(tb_callback)\n","\n","# Early Stopping\n","# --------------\n","early_stop = False\n","if early_stop:\n","    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=8)\n","    callbacks.append(es_callback)\n"],"execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.fit(train_dataset,\n","          epochs=50,\n","          steps_per_epoch=len(dataset)//32,\n","          validation_data=valid_dataset,\n","          validation_steps=len(dataset_valid)// 32, \n","          #validation_split=0.2, \n","          callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n 123/1470 [=>............................] - ETA: 5:35 - loss: 3.4805 - accuracy: 0.1242","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.load_weights('/kaggle/working/vqa_exps/exp_Jan31_09-53-13/ckpts/cp_40.ckpt')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Evaluation"]},{"metadata":{"trusted":true},"cell_type":"code","source":["with open(test_json_path) as data_file:    \n","    data_test_final = json.load(data_file)\n","data_file.close()\n","\n","csv_fname = 'results_'\n","csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","with open(os.path.join('./', csv_fname), 'w') as f:\n","    f.write('Id,Category\\n')\n","    for item in data_test_final.items():\n","        obj_value = item[0] #id\n","   \n","        image_id = item[1]['image_id']\n","        image_name = image_id + \".png\"\n","        img = Image.open(os.path.join(imgs_path, image_name))\n","        img = img.convert('RGB')\n","        img_arr = np.asarray(img)\n","        img_arr=img_arr[None,...]\n","\n","        question = item[1]['question']\n","        question_list = []\n","        question_list.append(question)\n","        question_tokenized = question_tokenizer.texts_to_sequences(question_list)\n","        question_padded = pad_sequences(question_tokenized, maxlen=max_question_length)\n","        question_tokenized = question_padded[0]\n","        question_tokenized = question_tokenized[None,...]\n","        \n","        input_net = [np.float32(question_tokenized), np.float32(img_arr)]\n","        prediction = np.argmax(model.predict(input_net))\n","        f.write(obj_value + ',' + str(prediction) + '\\n')\n","        \n","print('FINE')\n","        "],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}