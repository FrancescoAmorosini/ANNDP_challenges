{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nSEED = 1234\ntf.random.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dir = \"../input/anndl-2020-vqa/VQA_Dataset\"\n\nimgs_path = os.path.join(dataset_dir, \"Images\")\noutput_path = '/kaggle/working/'\ntrain_json_path = os.path.join(dataset_dir, \"train_questions_annotations.json\")\ntest_json_path = '../input/anndl-2020-vqa/VQA_Dataset/test_questions.json'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# andiamo a definire la divisione train val nei file txt\n\n\"\"\"per ogni ogg json prendo obj id (primo elemento) lo metto in una lista\npoi prendo la lista e faccio shuffle\ncreo txt train e val (e li svuoto)\npoi prendo i primi 80 % elem della lista e li scrivo nel txt train, il 20 % in txt val\"\"\"\n\n# 80:100=x:len x=lenx80:100\nimport os\nimport random\nimport json\n\nlist_obj_id = [] \nwith open(train_json_path) as data_file:    \n    data = json.load(data_file)\ndata_file.close()\n#i = 0\nfor item in data.items():\n    #i = i+1\n    obj_value = item[0]\n    list_obj_id.append(obj_value)\n    #print(obj_value)\n\nrandom.shuffle(list_obj_id)\nlenght = len(list_obj_id)\n#print(lenght, i)\n\nlenght_train = 80\nlenght_train = int((lenght * lenght_train)/100)\n\nsplit_dir = os.path.join(\"./\", \"Splits\")\n\nif not os.path.isdir(split_dir):\n    os.mkdir(split_dir)\n\ntrain_file= open(os.path.join(\"./\", 'Splits/train.txt'),\"w+\")\ntrain_file.truncate(0)\nval_file= open(os.path.join(\"./\", 'Splits/val.txt'),\"w+\")\nval_file.truncate(0)\n\nfor i in range (0, lenght_train-1):\n    elem = list_obj_id[0]\n    train_file.write(elem + \"\\n\")\n    list_obj_id.remove(elem)\n\ntrain_file.close()\n\nlenght = len(list_obj_id)\nfor i in range (0, lenght):\n    elem = list_obj_id[0]\n    val_file.write(elem + \"\\n\")\n    list_obj_id.remove(elem)\n\nval_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = {\n    '0': 0,\n    '1': 1,\n    '2': 2,\n    '3': 3,\n    '4': 4,\n    '5': 5,\n    'apple': 6,\n    'baseball': 7,\n    'bench': 8,\n    'bike': 9,\n    'bird': 10,\n    'black': 11,\n    'blanket': 12,\n    'blue': 13,\n    'bone': 14,\n    'book': 15,\n    'boy': 16,\n    'brown': 17,\n    'cat': 18,\n    'chair': 19,\n    'couch': 20,\n    'dog': 21,\n    'floor': 22,\n    'food': 23,\n    'football': 24,\n    'girl': 25,\n    'grass': 26,\n    'gray': 27,\n    'green': 28,\n    'left': 29,\n    'log': 30,\n    'man': 31,\n    'monkey bars': 32,\n    'no': 33,\n    'nothing': 34,\n    'orange': 35,\n    'pie': 36,\n    'plant': 37,\n    'playing': 38,\n    'red': 39,\n    'right': 40,\n    'rug': 41,\n    'sandbox': 42,\n    'sitting': 43,\n    'sleeping': 44,\n    'soccer': 45,\n    'squirrel': 46,\n    'standing': 47,\n    'stool': 48,\n    'sunny': 49,\n    'table': 50,\n    'tree': 51,\n    'watermelon': 52,\n    'white': 53,\n    'wine': 54,\n    'woman': 55,\n    'yellow': 56,\n    'yes': 57\n}\n\nN_CLASSES = len(classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Preprocessing****\n\nHere we do some preprocessing on the questions/answers sentences using a Tokenizer to encode them."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(train_json_path) as data_file:    \n    data = json.load(data_file)         \ndata_file.close()\n\nwith open(test_json_path) as dataTest_file:    \n    data_test = json.load(dataTest_file)         \ndataTest_file.close()\n\nquestions_sentences = [] # input encoder \nanswers_sentences = [] # target sentences\n    \nfor item in data.items():\n    question = item[1][\"question\"]\n    question = question.replace(\"?\", \"\")\n    questions_sentences.append(question)\n    \nfor item_test in data_test.items():\n    question = item[1][\"question\"]\n    question = question.replace(\"?\", \"\")\n    questions_sentences.append(question)\n\n# QUESTIONS PAD\nquestion_tokenizer = Tokenizer()\nquestion_tokenizer.fit_on_texts(questions_sentences)\nquestion_tokenized = question_tokenizer.texts_to_sequences(questions_sentences)\nquestion_wtoi = question_tokenizer.word_index\nmax_question_length = max(len(sentence) for sentence in question_tokenized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In CustomDataset_ we generate our tuples of question-image and answer to train our network. We have used a dictionary with question, image_id and answer as key and the question_encoded, the image_id and the answer_label as values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\nclass CustomDataset_(tf.keras.utils.Sequence):\n    def __init__(self, dataset_dir, which_subset, json_file, classes, n_classes, \n                 question_tokenizer, max_question_length): \n        \n        if which_subset == 'training':\n            subset_file = os.path.join('Splits', 'train.txt')\n        elif which_subset == 'validation':\n            subset_file = os.path.join('Splits', 'val.txt')\n        \n        with open(subset_file, 'r') as f:\n            lines = f.readlines()\n            \n        subset_filenames = []\n        for line in lines:\n            subset_filenames.append(line.strip()) \n            self.which_subset = which_subset\n            self.dataset_dir = dataset_dir # dir img\n            self.subset_filenames = subset_filenames\n    \n        with open(json_file) as data_file:    \n            data = json.load(data_file)         \n        data_file.close()\n        \n        data_copy=data.copy()\n        data_sub={k: data_copy[k] for k in subset_filenames}\n        for item in data_sub.items():\n            question = item[1][\"question\"]\n            question = question.replace(\"?\", \"\")\n            answer = item[1][\"answer\"]\n            questions_sentences.append(question)\n            answers_sentences.append(answer)\n\n        # QUESTIONS PAD\n        question_tokenized = question_tokenizer.texts_to_sequences(questions_sentences)\n        question_wtoi = question_tokenizer.word_index\n        question_padded = pad_sequences(question_tokenized, maxlen=max_question_length)\n\n        for e,item in enumerate(data_sub.items()):\n    \n            data_sub[item[0]]['question']=question_padded[e]\n            data_sub[item[0]]['answer']=answers_sentences[e]\n        \n        self.classes = classes\n        self.n_classes = n_classes\n        self.data = data\n        self.data_sub=data_sub\n        self.question_tokenized = question_tokenized\n        self.max_question_length = max_question_length\n        self.question_tokenizer=question_tokenizer\n\n    def __len__(self):\n        return len(self.subset_filenames)\n\n    def __getitem__(self, index):\n        curr_filename = self.subset_filenames[index]\n        obj = self.data_sub[curr_filename]\n\n        image_id = obj['image_id']\n        image_name = image_id + \".png\"\n        img = Image.open(os.path.join(self.dataset_dir, image_name))\n        img = img.convert('RGB')\n        img_arr = np.asarray(img)\n        img_arr=img_arr[None,...]\n\n        question_tokenized = obj['question']\n        question_tokenized = question_tokenized[None,...]\n         \n        answer = obj['answer']\n        answer_num = self.classes[answer]\n        n= np.zeros(1)\n        n[0] = answer_num\n        answer = n[None,...]\n\n        return (np.float32(question_tokenized), np.float32(img_arr)), np.float32(answer) #answer_tokenized\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Dataset Creation****\n\nWe create our training set and validation set similarly as we have done in the segmentation challenge, using the CustomDataset_ class."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dir = \"../input/anndl-2020-vqa/VQA_Dataset\"\nimg_h = 400\nimg_w = 700\nnum_classes=58\n\ndataset = CustomDataset_(imgs_path, 'training', train_json_path, classes, num_classes, \n                         question_tokenizer, max_question_length)\n\ndataset_valid = CustomDataset_(imgs_path, 'validation', train_json_path, classes, num_classes,\n                            question_tokenizer, max_question_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n                                               output_types=((tf.int32, tf.float32), tf.float32),\n                                               output_shapes=(([None, dataset.max_question_length], [None, img_h, img_w, 3]), [None, 1]))#None,img_h, img_w, 3\n\ntrain_dataset = train_dataset.repeat()\n\nvalid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n                                               output_types=((tf.int32, tf.float32), tf.float32),\n                                               output_shapes=(([None, dataset_valid.max_question_length], [None, img_h, img_w, 3]), [None, 1]))\n\nvalid_dataset = valid_dataset.repeat()\n\ntrain_dataset\n\nMAX_LENGTH=dataset.max_question_length\nprint(MAX_LENGTH)\n\nwords_number=(len(question_wtoi)+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Model****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport json\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nstart_f = 10\ndepth = 5\n\n# Define CNN for Image Input\n\nmodel = tf.keras.Sequential()\nfor i in range(depth):\n\n    if i == 0:\n        input_shape = [img_h, img_w, 3]\n    else:\n        input_shape=[None]\n\n    # Conv block: Conv2D -> Activation -> Pooling\n    model.add(tf.keras.layers.Conv2D(filters=start_f, \n                                     kernel_size=(3, 3), # invece di 3x3\n                                     strides=(1, 1),\n                                     padding='same',\n                                     input_shape=input_shape))\n    model.add(tf.keras.layers.LeakyReLU()) # provo leaky relu e non relu\n    #if i!=(depth-1):\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n    start_f *= 2\n    \n# Classifier\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(units=512, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(0.001)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(units=512, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l2(0.001)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(200))\n\nimage_input = tf.keras.layers.Input(shape=(img_h, img_w, 3))\nencoded_image = model(image_input)\n\n# Define RNN for language input\nquestion_input = tf.keras.layers.Input(shape=[MAX_LENGTH])\nembedded_question = tf.keras.layers.Embedding(input_dim=words_number, output_dim=512, input_length=MAX_LENGTH)(question_input)\nencoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.3, return_sequences=True, recurrent_dropout=0.1, unroll=True))(embedded_question)\nencoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, dropout=0.1, recurrent_dropout=0.1, unroll=True))(encoded_question)\n# Combine CNN and RNN to create the final model\nmerged = tf.keras.layers.concatenate([encoded_question, encoded_image])\noutput = tf.keras.layers.Dense(128)(merged)\noutput = tf.keras.layers.Dropout(0.1)(output)\noutput = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(output)\nvqa_model = tf.keras.models.Model(inputs=[question_input, image_input], outputs=output)\n\nvqa_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimization params\n# -------------------\n\n# Loss\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# learning rate\nlr = 10e-4\n#lr = 1e-6 #8\n#optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=lr, rho=0.9)\n\n# -------------------\n\n# Compile Model\n\nvqa_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nbs=32\n\ncwd = os.getcwd()\n\nexps_dir = os.path.join(cwd, 'vqa_exps')\nif not os.path.exists(exps_dir):\n    os.makedirs(exps_dir)\n\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\nexp_name = 'exp'\n\nexp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n    \ncallbacks = []\n\n# Model checkpoint\n# ----------------\nckpt_dir = os.path.join(exp_dir, 'ckpts')\nif not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n                                                   save_weights_only=True) \ncallbacks.append(ckpt_callback)\n\n# ----------------\n\n# Visualize Learning on Tensorboard\n# ---------------------------------\ntb_dir = os.path.join(exp_dir, 'tb_logs')\nif not os.path.exists(tb_dir):\n    os.makedirs(tb_dir)\n    \n# By default shows losses and metrics for both training and validation\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n                                             profile_batch=0,\n                                             histogram_freq=1) \ncallbacks.append(tb_callback)\n\n# Early Stopping\n# --------------\nearly_stop = False\nif early_stop:\n    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=7)\n    callbacks.append(es_callback)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vqa_model.fit(train_dataset,\n          epochs=100,\n          #batch_size=500,\n          steps_per_epoch=len(dataset)/bs,\n          validation_data=valid_dataset,\n          validation_steps = 1000, \n          #validation_split=0.2, \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vqa_model.load_weights('/kaggle/working/vqa_exps/exp_Jan28_16-08-27/ckpts/cp_06.ckpt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Evaluation****"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(test_json_path) as data_file:    \n    data_test_final = json.load(data_file)\ndata_file.close()\n\ncsv_fname = 'results_'\ncsv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\nwith open(os.path.join('./', csv_fname), 'w') as f:\n    f.write('Id,Category\\n')\n    for item in data_test_final.items():\n        obj_value = item[0] #id\n        image_id = item[1]['image_id']\n        image_name = image_id + \".png\"\n        img = Image.open(os.path.join(imgs_path, image_name))\n        img = img.convert('RGB')\n        img_arr = np.asarray(img)\n        img_arr=img_arr[None,...]\n\n        question = item[1]['question']\n        question_list = []\n        question_list.append(question)\n        question_tokenized = question_tokenizer.texts_to_sequences(question_list)\n        question_padded = pad_sequences(question_tokenized, maxlen=max_question_length)\n        \n        question_tokenized = question_padded[0]\n        question_tokenized = question_tokenized[None,...]\n        \n        input_net = [np.float32(question_tokenized), np.float32(img_arr)]\n        prediction = np.argmax(vqa_model.predict(input_net))\n        f.write(obj_value + ',' + str(prediction) + '\\n')\n        \nprint('FINE')\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}